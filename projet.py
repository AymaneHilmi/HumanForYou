# Import des bibliothÃ¨ques principales
import numpy as np
import io
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.figure_factory as ff
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, recall_score, roc_auc_score, roc_curve
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler
from sklearn.compose import ColumnTransformer
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier

# ğŸ“Œ CONFIGURATION DE L'INTERFACE
st.set_page_config(page_title="HumanForYou", layout="wide")

# ğŸ“Œ CHARGEMENT DES DONNÃ‰ES
@st.cache_data
def load_data():
    # Chargement des donnÃ©es
    hr_data = pd.read_csv('./data/general_data.csv')
    survey_data = pd.read_csv('./data/employee_survey_data.csv')
    manager_data = pd.read_csv('./data/manager_survey_data.csv')

    # Fusion des datasets
    hr_data = hr_data.merge(survey_data, on='EmployeeID')
    hr_data = hr_data.merge(manager_data, on='EmployeeID')
    
    # Nettoyage et conversions
    hr_data.drop(['EmployeeCount', 'StandardHours', 'Over18'], axis=1, inplace=True)
    hr_data.dropna(subset=['NumCompaniesWorked', 'TotalWorkingYears'], inplace=True)

    # Remplacement des valeurs manquantes
    for col in ['EnvironmentSatisfaction', 'JobSatisfaction', 'WorkLifeBalance']:
        hr_data[col] = hr_data[col].fillna(hr_data[col].median())

    # Transformation des variables catÃ©goriques
    hr_data['Age'] = hr_data['Age'].astype(int)
    hr_data['Attrition'] = hr_data['Attrition'].map({'Yes': 1, 'No': 0})
    hr_data['BusinessTravel'] = hr_data['BusinessTravel'].map({'Non-Travel': 0, 'Travel_Rarely': 1, 'Travel_Frequently': 2})
    hr_data['DistanceFromHome'] = hr_data['DistanceFromHome'].astype(int)
    hr_data['Education'] = hr_data['Education'].astype(int)
    hr_data['EducationField'] = hr_data['EducationField'].astype('category')
    hr_data['EmployeeID'] = hr_data['EmployeeID'].astype(int)
    hr_data['Gender'] = hr_data['Gender'].map({'Male': 1, 'Female': 0})
    hr_data['JobLevel'] = hr_data['JobLevel'].astype(int)
    hr_data['JobRole'] = hr_data['JobRole'].astype('category')
    hr_data['MaritalStatus'] = hr_data['MaritalStatus'].map({'Single': 0, 'Married': 1, 'Divorced': 2})
    hr_data['MonthlyIncome'] = hr_data['MonthlyIncome'].astype(float)
    hr_data['NumCompaniesWorked'] = hr_data['NumCompaniesWorked'].astype(int)
    hr_data['PercentSalaryHike'] = hr_data['PercentSalaryHike'].astype(float) / 100
    hr_data['StockOptionLevel'] = hr_data['StockOptionLevel'].astype(int)
    hr_data['TotalWorkingYears'] = hr_data['TotalWorkingYears'].astype(int)
    hr_data['TrainingTimesLastYear'] = hr_data['TrainingTimesLastYear'].astype(int)
    hr_data['YearsAtCompany'] = hr_data['YearsAtCompany'].astype(int)
    hr_data['YearsSinceLastPromotion'] = hr_data['YearsSinceLastPromotion'].astype(int)
    hr_data['YearsWithCurrManager'] = hr_data['YearsWithCurrManager'].astype(int)
    hr_data['JobInvolvement'] = hr_data['JobInvolvement'].astype(int)
    hr_data['PerformanceRating'] = hr_data['PerformanceRating'].astype(int)
    hr_data['EnvironmentSatisfaction'] = hr_data['EnvironmentSatisfaction'].astype(int)
    hr_data['WorkLifeBalance'] = hr_data['WorkLifeBalance'].astype(int)
    print(hr_data['MonthlyIncome'])
    hr_data["MonthlyIncome"] = hr_data["MonthlyIncome"].apply(lambda x: round(x, -3)) 
    print(hr_data['MonthlyIncome'])
    # Chargement des donnÃ©es d'absentÃ©isme
    in_time_data = pd.read_csv('./data/in_time.csv')
    out_time_data = pd.read_csv('./data/out_time.csv')
    in_time_data.rename(columns={"Unnamed: 0": 'EmployeeID'}, inplace=True)
    out_time_data.rename(columns={"Unnamed: 0": 'EmployeeID'}, inplace=True)

    # Calcul des jours d'absence
    absence_status = (in_time_data.iloc[:, 1:].isna() | out_time_data.iloc[:, 1:].isna())
    absence_status = absence_status.dropna(axis=0, how='all')
    absence_status = absence_status.replace({True: 'Absent', False: 'Present'})
    absence_status.insert(0, 'EmployeeID', in_time_data['EmployeeID'])
    
    # Comptage des jours d'absence
    absence_days = absence_status.iloc[:, 1:].apply(lambda x: (x == 'Absent').sum(), axis=1)
    absence_days = pd.DataFrame({'EmployeeID': absence_status['EmployeeID'], 'AbsenceDays': absence_days})
    
    hr_data = hr_data.merge(absence_days, on='EmployeeID', how='left')  # Ajouter le nombre de jours d'absence

    hr_data["CareerGrowthRate"] = hr_data["JobLevel"] / (hr_data["TotalWorkingYears"] + 1)
    hr_data["PromotionRate"] = hr_data["YearsSinceLastPromotion"] / (hr_data["YearsAtCompany"] + 1)
    hr_data["ManagerChangeRate"] = hr_data["YearsAtCompany"] / (hr_data["YearsWithCurrManager"] + 1)
    hr_data["SatisfactionScore"] = (hr_data["JobSatisfaction"] + hr_data["EnvironmentSatisfaction"] + hr_data["WorkLifeBalance"]) / 3
    hr_data["SalarySatisfactionGap"] = hr_data["MonthlyIncome"] / (hr_data["JobSatisfaction"] + 1)
    hr_data["PerformanceInvolvementGap"] = hr_data["PerformanceRating"] - hr_data["JobInvolvement"]
    hr_data["AbsenceRate"] = hr_data["AbsenceDays"] / (hr_data["YearsAtCompany"] + 1)
    hr_data["TravelFatigue"] = hr_data["BusinessTravel"] * hr_data["DistanceFromHome"]
    categorical_columns = ['Departement', 'EducationField', 'JobRole']
    binary_columns = ['Attrition', 'Gender']
    numerical_columns = hr_data.select_dtypes(include=['int64', 'float64']).columns.tolist()
    numerical_columns = [col for col in numerical_columns if col not in categorical_columns + binary_columns]
    scaler = MinMaxScaler()
    normalized_df = hr_data.copy()
    normalized_df[numerical_columns] = scaler.fit_transform(hr_data[numerical_columns])

    return hr_data, absence_status, absence_days, normalized_df

# Charger les donnÃ©es
df, absence_status, absence_days, normalized_df = load_data()

page1, page2, page3, page4, page5 = st.tabs(["Accueil","Analyse UnivariÃ©e", "Analyse BivariÃ©e & MultivariÃ©e", "Analyse AvancÃ©e & Business Insights", "PrÃ©diction"])

with page1 :
    # ğŸ“Œ TITRE PRINCIPAL
    st.title("ğŸ“Š HumanForYou - Dashboard")

    # ğŸ“Œ STATISTIQUES GÃ‰NÃ‰RALES
    st.subheader("ğŸ“Œ Statistiques ClÃ©s")
    col1, col2, col3 = st.columns(3)

    with col1:
        st.metric("ğŸŒ Nombre total d'employÃ©s", df.shape[0])
        st.metric("ğŸš€ Taux d'attrition", f"{df['Attrition'].mean() * 100:.2f} %")
        
    with col2:
        st.metric("ğŸ“ˆ Salaire moyen", f"${df['MonthlyIncome'].mean():,.2f}")
        st.metric("ğŸ“… AnciennetÃ© moyenne", f"{df['YearsAtCompany'].mean():.1f} ans")
        
    with col3:
        st.metric("ğŸ‘¨â€ğŸ’¼ % Hommes", f"{df[df['Gender'] == 1].shape[0] / df.shape[0] * 100:.1f} %")
        st.metric("ğŸ‘© % Femmes", f"{df[df['Gender'] == 0].shape[0] / df.shape[0] * 100:.1f} %")


    # ğŸ“Œ FONCTION POUR AFFICHER LES INDICATEURS AVEC LABELS VISUELS
    def display_metric(label, value, low_threshold, high_threshold):
        """Affiche un KPI avec une Ã©valuation visuelle : ğŸ”´ Mauvais, ğŸŸ¡ Moyen, ğŸŸ¢ Bon"""
        if value < low_threshold:
            status = "ğŸ”´ Mauvais"
        elif value < high_threshold:
            status = "ğŸŸ¡ Moyen"
        else:
            status = "ğŸŸ¢ Bon"
        st.metric(label, f"{value:.2f}", status)
    st.subheader("ğŸ“Œ Indicateurs de Performance et de Satisfaction")

    # ğŸ“Œ AFFICHAGE DES MÃ‰TRIQUES AVEC INDICATEURS
    col1, col2, col3 = st.columns(3)
    
    with col1:
        display_metric("ğŸ“ˆ Taux de Croissance de CarriÃ¨re", df['CareerGrowthRate'].mean(), 0.1, 0.5)
        display_metric("ğŸ“Š Taux de Promotion", df['PromotionRate'].mean(), 0.05, 0.2)
        display_metric("ğŸ”„ Changement de Manager", df['ManagerChangeRate'].mean(), 0.2, 0.8)

    with col2:
        display_metric("ğŸ˜Š Score Satisfaction", df['SatisfactionScore'].mean(), 2.0, 3.5)
        display_metric("ğŸ’° Ã‰cart Salaire/Satisfaction", df['SalarySatisfactionGap'].mean(), 3000, 8000)
        display_metric("ğŸ“‰ Performance - Implication", df['PerformanceInvolvementGap'].mean(), -1, 1)

    with col3:
        display_metric("ğŸšª Taux d'Absence", df['AbsenceRate'].mean(), 0.05, 0.2)
        display_metric("âœˆï¸ Fatigue liÃ©e au Voyage", df['TravelFatigue'].mean(), 5, 20)

    # ğŸ“Œ STATISTIQUES D'ABSENTÃ‰ISME
    st.subheader("ğŸ“Œ Statistiques d'AbsentÃ©isme")
    col1, col2 = st.columns(2)

    with col1:
        st.metric("ğŸ“Š Absence moyenne par employÃ©", f"{absence_days['AbsenceDays'].mean():.1f} jours")

    with col2:
        max_absences_employee = absence_days.loc[absence_days['AbsenceDays'].idxmax()]
        st.metric("ğŸ‘¥ EmployÃ© avec le plus d'absences", f"ID :{max_absences_employee['EmployeeID']} avec {max_absences_employee['AbsenceDays']} jours")

    # ğŸ‘¥ PrÃ©sentation des contributeurs
    st.subheader("ğŸ‘¨â€ğŸ’» Ã‰quipe Projet")
    
    team_members = [
        {"name": "ğŸ”¹ **Aymane Hilmi**"},
        {"name": "ğŸ”¹ **Clement FORNES**"},
        {"name": "ğŸ”¹ **Teo EMIROT**"},
        {"name": "ğŸ”¹ **Mathys MICHEL**"}
    ]

    for member in team_members:
        st.markdown(f"{member['name']}")

with page2 :
    # ğŸ“Œ TITRE PRINCIPAL
    st.title("ğŸ“Š Analyse des DonnÃ©es")
    # ğŸ“Œ ONGLETS INTERACTIFS 
    tab1, tab2, tab3 = st.tabs(["ğŸ“ˆ Statistiques GÃ©nÃ©rales", "ğŸ“Š Visualisation Dynamique", "ğŸ“ DonnÃ©es brutes"])

    with tab1:
        st.markdown("## ğŸ“Š Analyse UnivariÃ©e")
        st.markdown("#### Exploration des statistiques et rÃ©partition des donnÃ©es")

        # === Affichage des Statistiques GÃ©nÃ©rales ===
        st.subheader("ğŸ“Œ Statistiques GÃ©nÃ©rales")
        
        col1, col2 = st.columns([1, 2])  # SÃ©paration en 2 colonnes
        with col1:
            # ğŸ“Œ Transformer df.info() en DataFrame
            info_dict = {
                "Column": df.columns,
                "Non-Null Count": df.count().values,
                "Dtype": [df[col].dtype for col in df.columns]
            }
            df_info = pd.DataFrame(info_dict)

            # ğŸ“Š Affichage stylÃ©
            st.dataframe(df_info, height=500) 
        with col2:
            st.dataframe(df.describe(), height=300)  # Affichage des stats descriptives

        st.markdown("---")

        # === RÃ©partition des employÃ©s par dÃ©partement ===
        st.subheader("ğŸ¢ RÃ©partition des employÃ©s par dÃ©partement")
        department_counts = df['Department'].value_counts()

        col1, col2 = st.columns([1, 2])
        with col1:
            st.write(department_counts)

        with col2:
            fig, ax = plt.subplots(figsize=(8, 4))
            sns.barplot(y=department_counts.index, x=department_counts.values, palette="Blues_r", ax=ax)
            ax.set_xlabel("Nombre d'employÃ©s")
            ax.set_ylabel("DÃ©partement")
            ax.set_title("ğŸ“Š RÃ©partition par DÃ©partement")
            st.pyplot(fig)


        # === RÃ©partition des valeurs manquantes ===
        st.subheader("ğŸš¨ Gestion des valeurs manquantes")

        missing_values = df.isnull().sum()
        missing_values = missing_values[missing_values > 0].sort_values(ascending=False)

        if missing_values.empty:
            st.success("âœ… Aucune valeur manquante dÃ©tectÃ©e ! Tout est propre ğŸ‰")
        else:
            st.warning("âš ï¸ Certaines colonnes contiennent des valeurs manquantes.")

            # Affichage des valeurs manquantes sous forme de barplot
            st.subheader("ğŸ“‰ Distribution des valeurs manquantes")
            fig, ax = plt.subplots(figsize=(8, 4))
            sns.barplot(x=missing_values.index, y=missing_values.values, palette="Reds", ax=ax)
            ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')
            ax.set_ylabel("Nombre de valeurs manquantes")
            ax.set_title("ğŸ” Colonnes concernÃ©es")
            st.pyplot(fig)

            # Heatmap des valeurs manquantes
            st.subheader("ğŸ—ºï¸ Carte de chaleur des valeurs manquantes")
            fig, ax = plt.subplots(figsize=(10, 6))
            sns.heatmap(df.isnull(), cmap="Reds", cbar=False, yticklabels=False, ax=ax)
            ax.set_title("ğŸ” Heatmap des valeurs manquantes")
            st.pyplot(fig)

        st.markdown("---")

        st.subheader("ğŸ“Œ Indicateurs de Performance et de Satisfaction")

        # ğŸ“Œ FONCTION POUR AFFICHER LES INDICATEURS AVEC LABELS VISUELS
        def display_metric(label, value, low_threshold, high_threshold):
            """Affiche un KPI avec une Ã©valuation visuelle : ğŸ”´ Mauvais, ğŸŸ¡ Moyen, ğŸŸ¢ Bon"""
            if value < low_threshold:
                status = "ğŸ”´ Mauvais"
            elif value < high_threshold:
                status = "ğŸŸ¡ Moyen"
            else:
                status = "ğŸŸ¢ Bon"
            st.metric(label, f"{value:.2f}", status)

        # ğŸ“Œ AFFICHAGE DES MÃ‰TRIQUES AVEC INDICATEURS
        col1, col2, col3 = st.columns(3)
        
        with col1:
            display_metric("ğŸ“ˆ Taux de Croissance de CarriÃ¨re", df['CareerGrowthRate'].mean(), 0.1, 0.5)
            display_metric("ğŸ“Š Taux de Promotion", df['PromotionRate'].mean(), 0.05, 0.2)
            display_metric("ğŸ”„ Changement de Manager", df['ManagerChangeRate'].mean(), 0.2, 0.8)

        with col2:
            display_metric("ğŸ˜Š Score Satisfaction", df['SatisfactionScore'].mean(), 2.0, 3.5)
            display_metric("ğŸ’° Ã‰cart Salaire/Satisfaction", df['SalarySatisfactionGap'].mean(), 3000, 8000)
            display_metric("ğŸ“‰ Performance - Implication", df['PerformanceInvolvementGap'].mean(), -1, 1)

        with col3:
            display_metric("ğŸšª Taux d'Absence", df['AbsenceRate'].mean(), 0.05, 0.2)
            display_metric("âœˆï¸ Fatigue liÃ©e au Voyage", df['TravelFatigue'].mean(), 5, 20)


        # === RÃ©partition des Ã¢ges ===
        st.subheader("ğŸ“Š Distribution des Ã¢ges")
        st.write("ğŸ“ˆ RÃ©partition des Ã¢ges des employÃ©s"
                "\nğŸ”´ 18 - 25 ans, ğŸ”µ 26 - 35 ans, ğŸŸ¢ 36 - 45 ans, ğŸŸ¡ 46 - 55 ans, ğŸŸ£ 56 - 65 ans")
        age_bins = pd.cut(df['Age'], bins=[18, 25, 35, 45, 55, 65], precision=0, right=False)
        age_bins_str = age_bins.astype(str)
        age_distribution = age_bins_str.value_counts().sort_index()
        age_distribution.index = age_distribution.index.str.replace('[', '').str.replace(')', '').str.replace(',', ' -')
        st.bar_chart(age_distribution)

        st.markdown("---")

        # === RÃ©partition des annÃ©es d'anciennetÃ© ===
        st.subheader("ğŸ“ˆ RÃ©partition des annÃ©es d'anciennetÃ©")
        # axe x : nombre d'annÃ©es, axe y : nombre d'employÃ©s
        st.bar_chart(df['YearsAtCompany'].value_counts())
        satisfaction_mapping = {
            'EnvironmentSatisfaction': 'Satisfaction de l\'environnement de travail',
            'JobSatisfaction': 'Satisfaction du travail',
            'WorkLifeBalance': 'Ã‰quilibre travail-vie personnelle'
        }
        st.markdown("---")


        # === RÃ©partition des salaires ===
        st.subheader("ğŸ’° RÃ©partition des salaires par tranche")
        salary_bins = pd.cut(df['MonthlyIncome'], bins=5, precision=0)
        salary_bins_str = salary_bins.astype(str)
        salary_distribution = salary_bins_str.value_counts().sort_index()
        salary_distribution.index = salary_distribution.index.str.replace('(', '').str.replace(']', '').str.replace(',', ' -')

        col1, col2 = st.columns([1, 2])
        with col1:
            st.write("ğŸ’¼ Distribution des salaires :")
            st.dataframe(salary_distribution)

        with col2:
            st.bar_chart(salary_distribution)

        st.markdown("---")

        

        # === Satisfaction des employÃ©s ===
        satisfaction_mapping = {
            'EnvironmentSatisfaction': 'Satisfaction de l\'environnement de travail',
            'JobSatisfaction': 'Satisfaction du travail',
            'WorkLifeBalance': 'Ã‰quilibre travail-vie personnelle'
        }

        st.subheader("ğŸ˜€ Satisfaction des employÃ©s")

        satisfaction_cols = ['EnvironmentSatisfaction', 'JobSatisfaction', 'WorkLifeBalance']

        for col in satisfaction_cols:
            st.write(f"### ğŸ“Š {satisfaction_mapping[col]}")
            
            # CrÃ©ation des colonnes pour une meilleure disposition
            col1, col2 = st.columns([1, 2])

            with col1:
                st.write("ğŸ“‹ RÃ©partition des niveaux :")
                st.dataframe(df[col].value_counts().rename_axis("Niveau").reset_index(name="Nombre d'employÃ©s"))

            with col2:
                st.write("ğŸ“Š Distribution graphique :")
                st.bar_chart(df[col].value_counts())
        st.markdown("---")

    with tab2:
        st.subheader("2ï¸âƒ£ Visualisation Dynamique des Variables")
        # SÃ©lection de la variable Ã  explorer
        selected_var = st.selectbox(
            "ğŸ“Š SÃ©lectionnez une variable numÃ©rique Ã  analyser :",
            df.select_dtypes(include=['int64', 'float64']).columns.tolist()
        )

        # CrÃ©ation des colonnes pour un affichage structurÃ©
        col1, col2 = st.columns([1, 2])

        # Histogramme interactif
        st.write("ğŸ“ **Distribution des valeurs (Histogramme)**")
        fig, ax = plt.subplots(figsize=(8, 3))
        sns.histplot(df[selected_var], kde=True, color="royalblue", ax=ax)
        ax.set_xlabel(selected_var)
        ax.set_ylabel("FrÃ©quence")
        st.pyplot(fig)

        # Boxplot interactif

        st.write("ğŸ“¦ **Diagramme a Moustache (Boxplot)**")
        fig, ax = plt.subplots(figsize=(6, 3))
        sns.boxplot(x=df[selected_var], color="lightcoral", ax=ax)
        ax.set_xlabel(selected_var)
        st.pyplot(fig)

        # KDE Plot interactif (Courbe de densitÃ©)
        st.write("ğŸ“Š **Courbe de densitÃ© (KDE Plot)**")
        fig, ax = plt.subplots(figsize=(8, 3))
        sns.kdeplot(df[selected_var], shade=True, color="green", ax=ax)
        ax.set_xlabel(selected_var)
        ax.set_ylabel("DensitÃ©")
        st.pyplot(fig)

        st.markdown("---")

    with tab3:
        st.subheader("ğŸ“‚ AperÃ§u des donnÃ©es")
        st.dataframe(df.head(20))

    # ğŸ“Œ TAB 4 : INDICATEURS DE PERFORMANCE

with page3:
    with st.expander("ğŸ” Options d'analyse", expanded=False):
        selected_features = st.multiselect("SÃ©lectionnez les variables Ã  afficher dans la matrice de corrÃ©lation :", 
                                       df.select_dtypes(include=['int64', 'float64']).columns.tolist(), 
                                       default=["Attrition","JobLevel", "YearsAtCompany", "YearsWithCurrManager",
                                                "YearsSinceLastPromotion", "NumCompaniesWorked", "MonthlyIncome",
                                                "PercentSalaryHike", "StockOptionLevel", "JobSatisfaction", "WorkLifeBalance",
                                                "EnvironmentSatisfaction", "TrainingTimesLastYear", "BusinessTravel",
                                                "DistanceFromHome", "AbsenceDays", "TotalWorkingYears",
                                                "PerformanceRating", "JobInvolvement"])

    # ğŸ“Œ MATRICE DE CORRÃ‰LATION INTERACTIVE
    st.subheader("ğŸ“Œ Matrice de CorrÃ©lation Interactive")

    # Filtrer les donnÃ©es selon les variables sÃ©lectionnÃ©es
    correlation_matrix = normalized_df[selected_features].corr()

    # âœ… **Correction de l'ordre des indices pour la diagonale correcte**
    correlation_matrix = correlation_matrix.iloc[::-1]  # Inverser l'ordre des lignes pour la bonne orientation

    # CrÃ©ation de la figure Plotly avec des couleurs modernes
    fig = ff.create_annotated_heatmap(
        z=correlation_matrix.values,
        x=list(correlation_matrix.columns),
        y=list(correlation_matrix.index)[::-1],  # Inverser l'ordre des colonnes pour correspondre
        colorscale="RdBu",  # Palette de couleurs moderne
        annotation_text=np.round(correlation_matrix.values, 2),
        showscale=True,
        reversescale=True
    )

    # Mise en page optimisÃ©e
    fig.update_layout(
        title="Matrice de CorrÃ©lation",
        xaxis=dict(title="Variables"),
        yaxis=dict(title="Variables"),
        margin=dict(l=100, r=100, t=50, b=50),
        height=700
    )

    # Affichage dans Streamlit
    st.plotly_chart(fig, use_container_width=True)

    # ğŸ“Œ ANALYSE DES DÃ‰PARTS (Comparaison EmployÃ©s Partis vs. Restants)
    st.subheader("ğŸ“Œ Comparaison des EmployÃ©s Partis vs. Restants")

    # Transformation des colonnes pour une meilleure lisibilitÃ©
    df["Gender"] = df["Gender"].map({1: "Homme", 0: "Femme"})
    df["MaritalStatus"] = df["MaritalStatus"].map({0: "CÃ©libataire", 1: "MariÃ©", 2: "DivorcÃ©"})

    # ğŸ“Œ COMPARAISON PAR FACTEUR CLÃ‰
    attrition_comparison = {
        "ğŸ’° Salaire Moyen": df.groupby("Attrition")["MonthlyIncome"].mean(),
        "ğŸ¢ AnnÃ©es dans l'Entreprise": df.groupby("Attrition")["YearsAtCompany"].mean(),
        "ğŸš€ DerniÃ¨re Augmentation (%)": df.groupby("Attrition")["PercentSalaryHike"].mean(),
        "ğŸ”„ Nombre d'Entreprises PrÃ©cÃ©dentes": df.groupby("Attrition")["NumCompaniesWorked"].mean(),
        "ğŸ“ˆ Niveau HiÃ©rarchique": df.groupby("Attrition")["JobLevel"].mean(),
        "ğŸ  Distance Domicile-Travail (km)": df.groupby("Attrition")["DistanceFromHome"].mean(),
        "ğŸ“Š Score Satisfaction": df.groupby("Attrition")["SatisfactionScore"].mean(),
        "ğŸ“ˆ Taux de Promotion": df.groupby("Attrition")["PromotionRate"].mean(),
        "ğŸšª Taux d'Absence": df.groupby("Attrition")["AbsenceRate"].mean(),
    }

    # ğŸ“Œ SÃ©lection du critÃ¨re de comparaison
    option = st.selectbox(
        "Choisissez un critÃ¨re d'analyse :", 
        list(attrition_comparison.keys())
    )

    # ğŸ“Š Fonction pour afficher le graphique comparatif
    def plot_attrition_chart(data, title):
        st.subheader(title)
        st.bar_chart(data)

    # ğŸ“Œ Affichage du graphique sÃ©lectionnÃ©
    plot_attrition_chart(attrition_comparison[option], option)

    # ğŸ“Œ INTERPRÃ‰TATION DES RÃ‰SULTATS
    st.subheader("ğŸ“Œ InterprÃ©tation des RÃ©sultats")

    st.write("ğŸ“Œ **EmployÃ©s Partis (Attrition = 1)**")
    st.write("ğŸ“Œ **EmployÃ©s Restants (Attrition = 0)**")

    st.subheader("ğŸ“‰ Analyse des employÃ©s ayant quittÃ© l'entreprise")
    col1, col2 = st.columns(2)

    with col1:
        st.write("ğŸ“Œ **Moyenne d'Ã¢ge des employÃ©s ayant quittÃ© :**")
        st.write(f"â¡ï¸ {df[df['Attrition'] == 1]['Age'].mean():.1f} ans")

        st.write("ğŸ“Œ **Salaire moyen des employÃ©s ayant quittÃ© :**")
        st.write(f"â¡ï¸ ${df[df['Attrition'] == 1]['MonthlyIncome'].mean():,.2f}")

    with col2:
        st.write("ğŸ“Œ **Nombre moyen d'annÃ©es dans l'entreprise avant de partir :**")
        st.write(f"â¡ï¸ {df[df['Attrition'] == 1]['YearsAtCompany'].mean():.1f} ans")

        st.write("ğŸ“Œ **Niveau moyen de satisfaction des employÃ©s ayant quittÃ© :**")
        st.write(f"â¡ï¸ {df[df['Attrition'] == 1]['JobSatisfaction'].mean():.1f} / 4")

with page4:
    #page 4
    st.write("ğŸ“Œ **Niveau moyen de satisfaction des employÃ©s ayant quittÃ© :**")

with page5:
    # ğŸ“Œ ONGLETS INTERACTIFS
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“Š RÃ©gression Logistique", "ğŸ§  SVM", "ğŸŒ² Random Forest", "ğŸŒ³ Decision Tree"])

    # ğŸ“Œ VARIABLES Ã€ UTILISER DANS LES MODÃˆLES
    features = [
        "JobRole", "JobLevel", "YearsAtCompany", "YearsWithCurrManager",
        "YearsSinceLastPromotion", "NumCompaniesWorked", "MonthlyIncome",
        "PercentSalaryHike", "JobSatisfaction", "WorkLifeBalance", "EnvironmentSatisfaction",
        "TrainingTimesLastYear",
        "BusinessTravel",
        "AbsenceDays",
        "TotalWorkingYears",
        "Department"
    ]
    with tab1:
        # ğŸ“Œ VARIABLES Ã€ UTILISER DANS LE MODÃˆLE
        features = [
            "JobRole", "JobLevel", "YearsAtCompany", "YearsWithCurrManager",
            "YearsSinceLastPromotion", "NumCompaniesWorked", "MonthlyIncome",
            "PercentSalaryHike", "StockOptionLevel", "JobSatisfaction", "WorkLifeBalance",
            "EnvironmentSatisfaction", "TrainingTimesLastYear", "BusinessTravel",
            "DistanceFromHome", "AbsenceDays", "TotalWorkingYears", "Department",
            "Education", "PerformanceRating", "JobInvolvement"
        ]

        target = "Attrition"  # Variable cible (1 = Quitte l'entreprise, 0 = Reste)

        # ğŸ“Œ PRÃ‰PARATION DES DONNÃ‰ES
        categorical_features = ["JobRole", "BusinessTravel", "Department"]
        numerical_features = [col for col in features if col not in categorical_features]

        # Encoder les variables catÃ©goriques
        encoder = OneHotEncoder(handle_unknown="ignore", sparse_output=False)
        df_encoded = pd.DataFrame(encoder.fit_transform(df[categorical_features]),
                                  columns=encoder.get_feature_names_out(categorical_features))
        df_encoded.index = df.index

        # Normaliser les variables numÃ©riques
        scaler = StandardScaler()
        df_scaled = pd.DataFrame(scaler.fit_transform(df[numerical_features]),
                                 columns=numerical_features)
        df_scaled.index = df.index

        # Combiner les donnÃ©es transformÃ©es
        df_final = pd.concat([df_encoded, df_scaled, df[target]], axis=1)

        # ğŸ“Œ DIVISION DES DONNÃ‰ES EN TRAIN & TEST
        X = df_final.drop(columns=[target])
        y = df_final[target]
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

        model = LogisticRegression(max_iter=500)
        model.fit(X_train, y_train)

        # ğŸ“Œ PRÃ‰DICTION & AJUSTEMENT DU SEUIL
        y_pred_proba = model.predict_proba(X_test)[:, 1]
        threshold = 0.35  # Ajustement du seuil
        y_pred = (y_pred_proba >= threshold).astype(int)

        # ğŸ“Œ Ã‰VALUATION DU MODÃˆLE
        accuracy = accuracy_score(y_test, y_pred)

        # ğŸ“Œ AFFICHAGE DES RÃ‰SULTATS DANS STREAMLIT
        st.subheader("ğŸ“Š PrÃ©diction de l'attrition avec RÃ©gression Logistique")
        st.write(f"ğŸ“Œ **PrÃ©cision du modÃ¨le :** {accuracy * 100:.2f} %")

        # ğŸ“Œ AFFICHAGE DE LA MATRICE DE CONFUSION
        conf_matrix = confusion_matrix(y_test, y_pred)
        fig, ax = plt.subplots(figsize=(5,3))
        sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues",
                    xticklabels=["Reste", "Part"], yticklabels=["Reste", "Part"])
        plt.xlabel("PrÃ©diction")
        plt.ylabel("RÃ©el")
        plt.title("Matrice de Confusion")
        st.pyplot(fig)

        # ğŸ“Œ FONCTION POUR AFFICHER LES STATISTIQUES DU MODÃˆLE
        def display_metrics(y_test, y_pred, model_name="RÃ©gression Logistique"):
            st.subheader(f"ğŸ“Š Performances du modÃ¨le : {model_name}")
            class_report = classification_report(y_test, y_pred, output_dict=True, zero_division=1)
            df_report = pd.DataFrame(class_report).transpose()
            st.dataframe(df_report)
            st.write(f"ğŸ“Œ **PrÃ©cision globale (Accuracy) :** {class_report['accuracy'] * 100:.2f} %")
            st.write(f"ğŸ“Œ **Score F1 (moyenne pondÃ©rÃ©e) :** {class_report['weighted avg']['f1-score']:.2f}")
            st.write(f"ğŸ“Œ **Rappel (Recall, capacitÃ© Ã  dÃ©tecter les partants) :** {class_report['1']['recall']:.2f}")
            st.write(f"ğŸ“Œ **PrÃ©cision (PrÃ©cision sur les employÃ©s rÃ©ellement partants) :** {class_report['1']['precision']:.2f}")

            # Calcul et affichage des taux de faux positifs et faux nÃ©gatifs
            FP_rate = conf_matrix[0, 1] / (conf_matrix[0, 1] + conf_matrix[0, 0])
            FN_rate = conf_matrix[1, 0] / (conf_matrix[1, 0] + conf_matrix[1, 1])
            st.write(f"ğŸ“Œ **Taux de Faux Positifs (False Positive Rate) :** {FP_rate:.2f}")
            st.write(f"ğŸ“Œ **Taux de Faux NÃ©gatifs (False Negative Rate) :** {FN_rate:.2f}")

        # ğŸ“Œ APPELER LA FONCTION POUR AFFICHER LES MÃ‰TRIQUES
        display_metrics(y_test, y_pred)

    with tab2:
        # Partie mathys
        st.write(f"SVM")
        # ============================
        # ğŸ“Œ MODÃˆLE DE PRÃ‰DICTION SVM 
        # ============================

        st.header("ModÃ¨le de PrÃ©diction SVM - OptimisÃ© pour dÃ©tecter les dÃ©parts")

        # CrÃ©er une copie du dataframe pour le modÃ¨le SVM
        df_svm = df.copy()

        # Conversion des variables de satisfaction en entier
        df_svm["JobSatisfaction"] = df_svm["JobSatisfaction"].astype(int)
        df_svm["EnvironmentSatisfaction"] = df_svm["EnvironmentSatisfaction"].astype(int)
        df_svm["WorkLifeBalance"] = df_svm["WorkLifeBalance"].astype(int)
        df_svm["SatisfactionScore"] = (df_svm["JobSatisfaction"] + df_svm["EnvironmentSatisfaction"] + df_svm["WorkLifeBalance"]) / 3
        df_svm["SalarySatisfactionGap"] = df_svm["MonthlyIncome"] / (df_svm["JobSatisfaction"] + 1)
        # Calcul de la diffÃ©rence entre PerformanceRating et JobInvolvement
        df_svm["PerformanceInvolvementGap"] = df_svm["PerformanceRating"].astype(int) - df_svm["JobInvolvement"].astype(int)
        df_svm["AbsenceRate"] = df_svm["AbsenceDays"] / (df_svm["YearsAtCompany"] + 1)
        # Calcul de TravelFatigue avant modification de BusinessTravel
        df_svm["TravelFatigue"] = df_svm["BusinessTravel"] * df_svm["DistanceFromHome"]
        # Encoder BusinessTravel en tant que variable catÃ©gorielle
        df_svm["BusinessTravel"] = df_svm["BusinessTravel"].astype(str)

        # --- DÃ©finition des features et de la variable cible ---
        features = [
            "JobRole", "JobLevel", "YearsAtCompany", "YearsWithCurrManager", "YearsSinceLastPromotion", "NumCompaniesWorked",
            "MonthlyIncome", "PercentSalaryHike",
            "JobSatisfaction", "WorkLifeBalance", "EnvironmentSatisfaction", "TrainingTimesLastYear",
            "BusinessTravel", "DistanceFromHome",
            "AbsenceDays",
            "TotalWorkingYears",
            "Department"
        ]
        target = "Attrition"

        X = df_svm[features].copy()
        y = df_svm[target]

        # --- Encodage des variables catÃ©gorielles ---
        # On encode "JobRole", "Department" et "BusinessTravel" via one-hot encoding
        X = pd.get_dummies(X, columns=["JobRole", "Department", "BusinessTravel"], drop_first=True)

        # --- Normalisation des donnÃ©es ---
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        X = pd.DataFrame(X_scaled, columns=X.columns)

        # --- SÃ©paration en ensembles d'entraÃ®nement et de test ---
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        # --- Initialisation et entraÃ®nement du modÃ¨le SVM ---
        # On utilise 'class_weight' pour compenser le dÃ©sÃ©quilibre et se concentrer sur les dÃ©parts (classe positive)
        # Les paramÃ¨tres sont fixÃ©s pour rÃ©duire le temps d'exÃ©cution
        svm_model = SVC(probability=True, random_state=42, class_weight='balanced', kernel='rbf', C=1, gamma=0.1)
        svm_model.fit(X_train, y_train)

        # --- PrÃ©dictions ---
        y_pred = svm_model.predict(X_test)

        # --- Ã‰valuation du modÃ¨le SVM ---
        conf_matrix = confusion_matrix(y_test, y_pred)
        conf_matrix_df = pd.DataFrame(conf_matrix, index=["Actual No", "Actual Yes"], columns=["Predicted No", "Predicted Yes"])

        report_dict = classification_report(y_test, y_pred, output_dict=True)
        report_df = pd.DataFrame(report_dict).transpose()

        y_proba = svm_model.predict_proba(X_test)[:, 1]
        roc_auc = roc_auc_score(y_test, y_proba)
        fpr, tpr, thresholds = roc_curve(y_test, y_proba)

        # Affichage des rÃ©sultats dans Streamlit avec une mise en forme pour une meilleure lisibilitÃ©
        st.subheader("RÃ©sultats du ModÃ¨le SVM - OptimisÃ© pour Attrition")

        st.markdown("**Matrice de Confusion :**")
        st.table(conf_matrix_df)

        st.markdown("**Rapport de Classification :**")
        st.table(report_df)

        st.markdown(f"**AUC-ROC :** {roc_auc:.4f}")

        # TracÃ© de la courbe ROC
        fig_roc, ax_roc = plt.subplots(figsize=(8, 6))
        ax_roc.plot(fpr, tpr, label=f"SVM (AUC = {roc_auc:.2f})")
        ax_roc.plot([0, 1], [0, 1], 'k--')
        ax_roc.set_xlabel("Taux de faux positifs")
        ax_roc.set_ylabel("Taux de vrais positifs")
        ax_roc.set_title("Courbe ROC - SVM OptimisÃ© pour Attrition")
        ax_roc.legend(loc="lower right")
        st.pyplot(fig_roc)

        # SÃ©lectionner uniquement les colonnes numÃ©riques du dataframe utilisÃ© pour le modÃ¨le
        df_corr = df_svm.select_dtypes(include=['number'])
        fig_corr, ax_corr = plt.subplots(figsize=(12, 10))
        sns.heatmap(df_corr.corr(), annot=True, fmt=".2f", cmap="coolwarm", ax=ax_corr)

        # --- Graphique des variables les plus corrÃ©lÃ©es avec l'Attrition ---
        st.subheader("Variables les plus corrÃ©lÃ©es avec l'Attrition")

        # S'assurer que la colonne 'Attrition' est de type numÃ©rique
        df_svm["Attrition"] = pd.to_numeric(df_svm["Attrition"], errors="coerce")

        # SÃ©lectionner uniquement les colonnes numÃ©riques du dataframe utilisÃ© pour le modÃ¨le
        df_corr = df_svm.select_dtypes(include=["number"])

        # VÃ©rifier si 'Attrition' est prÃ©sent dans df_corr
        if "Attrition" not in df_corr.columns:
            st.error("La colonne 'Attrition' n'est pas prÃ©sente dans les donnÃ©es numÃ©riques.")
        else:
            # Calculer la matrice de corrÃ©lation et extraire la corrÃ©lation avec Attrition
            corr_matrix = df_corr.corr()
            corr_attrition = corr_matrix["Attrition"].drop("Attrition")

            # SÃ©parer les corrÃ©lations positives et nÃ©gatives
            positive_corr = corr_attrition[corr_attrition > 0].sort_values(ascending=False)
            negative_corr = corr_attrition[corr_attrition < 0].sort_values()

            # Graphique pour les variables positivement corrÃ©lÃ©es (vertical bar chart)
            if not positive_corr.empty:
                fig_pos, ax_pos = plt.subplots(figsize=(8, 4))
                top_positive = positive_corr.head(5)
                top_positive.plot(kind="bar", ax=ax_pos, color="green")
                ax_pos.set_title("Top 5 variables positivement corrÃ©lÃ©es Ã  l'Attrition")
                ax_pos.set_xlabel("Variables")
                ax_pos.set_ylabel("Coefficient de corrÃ©lation")
                st.pyplot(fig_pos)
            else:
                st.write("Aucune corrÃ©lation positive trouvÃ©e.")

            # Graphique pour les variables nÃ©gativement corrÃ©lÃ©es (vertical bar chart)
            if not negative_corr.empty:
                fig_neg, ax_neg = plt.subplots(figsize=(8, 4))
                top_negative = negative_corr.head(5)
                top_negative.plot(kind="bar", ax=ax_neg, color="red")
                ax_neg.set_title("Top 5 variables nÃ©gativement corrÃ©lÃ©es Ã  l'Attrition")
                ax_neg.set_xlabel("Variables")
                ax_neg.set_ylabel("Coefficient de corrÃ©lation")
                st.pyplot(fig_neg)
            else:
                st.write("Aucune corrÃ©lation nÃ©gative trouvÃ©e.")


    with tab3:
        # Titre de l'application
        st.title("PrÃ©diction de l'Attrition avec Random Forest")
        st.markdown("Ce modÃ¨le utilise un Random Forest pour prÃ©dire si un employÃ© quittera l'entreprise (attrition).")
        # PrÃ©paration des donnÃ©es
        categorical_columns = ['Departement', 'EducationField', 'JobRole']
        binary_columns = ['Attrition', 'Gender']
        numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
        numerical_columns = [col for col in numerical_columns if col not in categorical_columns + binary_columns]
        scaler = MinMaxScaler()
        normalized_df = df.copy()
        normalized_df[numerical_columns] = scaler.fit_transform(df[numerical_columns])

        # SÃ©lection des variables pour la prÃ©diction
        features = [
            "JobRole", "JobLevel", "YearsAtCompany", "YearsWithCurrManager",
            "YearsSinceLastPromotion", "NumCompaniesWorked", "MonthlyIncome",
            "PercentSalaryHike", "JobSatisfaction", "WorkLifeBalance", "EnvironmentSatisfaction",
            "TrainingTimesLastYear",
            "BusinessTravel",
            "AbsenceDays",
            "TotalWorkingYears",
            "Department"]

        df_encoded = pd.get_dummies(df[features])

        X = df_encoded
        y = df['Attrition']

        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        # ğŸ“Œ EntraÃ®nement du modÃ¨le
        rf_model = RandomForestClassifier(random_state=42)
        rf_model.fit(X_train, y_train)

        # ğŸ“Œ Ã‰valuation du modÃ¨le
        rf_pred = rf_model.predict(X_test)
        rf_accuracy = accuracy_score(y_test, rf_pred)

        # Affichage des rÃ©sultats
        st.subheader("ğŸ“Š RÃ©sultats de la PrÃ©diction")
        st.write(f"**Accuracy :** {rf_accuracy}")
        st.write("PrÃ©diction sur l'ensemble de test :")
        st.write("ğŸ”´ 0 : Non Attrition, ğŸŸ¢ 1 : Attrition")

        # Recall
        st.write("**Recall :**", recall_score(y_test, rf_pred))
        st.write("**Classification Report :**")
        st.text(classification_report(y_test, rf_pred))

        # Matrice de confusion
        st.write("**Confusion Matrix :**")
        cm = confusion_matrix(y_test, rf_pred)
        fig, ax = plt.subplots()
        ax.matshow(cm, cmap='Blues', alpha=0.7)
        for (i, j), val in np.ndenumerate(cm):
            ax.text(j, i, val, ha='center', va='center', color='black')
        plt.xlabel('PrÃ©dictions')
        plt.ylabel('RÃ©el')
        st.pyplot(fig)

        # ğŸ“Œ Importance des Variables
        st.subheader("ğŸ“ˆ Importance des Variables")
        feature_importance = pd.Series(rf_model.feature_importances_, index=X.columns)
        feature_importance = feature_importance.sort_values(ascending=False)
        st.bar_chart(feature_importance.head(10))

        # ğŸ“Œ Conclusion
        st.write("L'importance des variables montre quelles caractÃ©ristiques influencent le plus la prÃ©diction d'attrition.")
        st.write("L'accuracy et le recall sont des mÃ©triques clÃ©s pour Ã©valuer la performance du modÃ¨le.")
        
    with tab4:
        st.subheader("ğŸŒ³ PrÃ©diction avec Decision Tree")

        # DÃ©finition des features et de la target pour le Decision Tree
        features = [
            "JobRole",
            "JobLevel",
            "YearsAtCompany",
            "YearsWithCurrManager",
            "YearsSinceLastPromotion",
            "NumCompaniesWorked",
            "MonthlyIncome",
            "PercentSalaryHike",
            "JobSatisfaction",
            "WorkLifeBalance",
            "EnvironmentSatisfaction",
            "TrainingTimesLastYear",
            "BusinessTravel",
            "AbsenceDays",
            "TotalWorkingYears"
        ]
        target = "Attrition"

        # SÃ©paration des variables catÃ©goriques et numÃ©riques
        categorical_features = ["JobRole", "BusinessTravel", "Department"]
        numerical_features = [col for col in features if col not in categorical_features]

        # CrÃ©ation d'un prÃ©processeur avec ColumnTransformer
        preprocessor = ColumnTransformer(transformers=[
            ('cat', OneHotEncoder(handle_unknown="ignore", sparse_output=False), categorical_features),
            ('num', StandardScaler(), numerical_features)
        ])

        # Transformation des donnÃ©es
        df_transformed = pd.DataFrame(preprocessor.fit_transform(df[categorical_features + numerical_features]),
                                      columns=preprocessor.get_feature_names_out(),
                                      index=df.index)
        # Combinaison avec la target
        df_final = pd.concat([df_transformed, df[target]], axis=1)

        # Division des donnÃ©es en ensembles d'entraÃ®nement et de test
        X = df_final.drop(columns=[target])
        y = df_final[target]
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

        # DÃ©finition de la grille de recherche pour le Decision Tree
        param_grid_dt = {
            'max_depth': [None, 5, 10, 15],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4],
            'max_features': [None, 'sqrt', 'log2'],
            'class_weight': [None, 'balanced']
        }

        grid_dt = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid_dt,
                               cv=5, scoring='f1', n_jobs=-1)
        grid_dt.fit(X_train, y_train)
        best_dt = grid_dt.best_estimator_

        st.write("### Meilleurs paramÃ¨tres pour Decision Tree")
        st.write(grid_dt.best_params_)

        # PrÃ©diction avec le meilleur modÃ¨le
        y_pred = best_dt.predict(X_test)
        accuracy_dt = accuracy_score(y_test, y_pred)

        st.write(f"ğŸ“Œ **PrÃ©cision du modÃ¨le Decision Tree optimisÃ© :** {accuracy_dt * 100:.2f} %")

        # Calcul de la matrice de confusion
        conf_matrix = confusion_matrix(y_test, y_pred)

        # Affichage de la matrice de confusion sous forme de heatmap
        fig_cm, ax_cm = plt.subplots(figsize=(5, 3))
        sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Greens",
                    xticklabels=["Reste", "Part"], yticklabels=["Reste", "Part"], ax=ax_cm)
        ax_cm.set_xlabel("PrÃ©diction")
        ax_cm.set_ylabel("RÃ©el")
        ax_cm.set_title("Matrice de Confusion")
        st.pyplot(fig_cm)


        # Fonction pour afficher les statistiques du modÃ¨le
        def display_metrics(y_true, y_pred, model_name="Decision Tree"):
            st.subheader(f"ğŸ“Š Performances du modÃ¨le : {model_name}")
            class_report = classification_report(y_true, y_pred, output_dict=True, zero_division=1)
            df_report = pd.DataFrame(class_report).transpose()
            st.dataframe(df_report)
            st.write(f"ğŸ“Œ **PrÃ©cision globale (Accuracy) :** {class_report['accuracy'] * 100:.2f} %")
            st.write(f"ğŸ“Œ **Score F1 (moyenne pondÃ©rÃ©e) :** {class_report['weighted avg']['f1-score']:.2f}")
            st.write(f"ğŸ“Œ **Rappel (Recall, capacitÃ© Ã  dÃ©tecter les partants) :** {class_report['1']['recall']:.2f}")
            st.write(
                f"ğŸ“Œ **PrÃ©cision (PrÃ©cision sur les employÃ©s rÃ©ellement partants) :** {class_report['1']['precision']:.2f}")


        # Affichage des mÃ©triques
        display_metrics(y_test, y_pred, model_name="Decision Tree OptimisÃ©")